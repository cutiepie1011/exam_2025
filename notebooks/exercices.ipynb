{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CACBFsndOCo"
      },
      "source": [
        "# Exercices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Préliminaires**: Clone de votre repo et imports"
      ],
      "metadata": {
        "id": "hfkMtaHleKAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/cutiepie1011/exam_2025.git\n",
        "! cp exam_2025/utils/utils_exercices.py .\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "xiD_cI-geJjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3add9333-a7a3-46c2-cd2c-fa4928e754c9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'exam_2025'...\n",
            "remote: Enumerating objects: 59, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 59 (delta 21), reused 20 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (59/59), 1.41 MiB | 3.53 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clef personnelle pour la partie théorique**\n",
        "\n",
        "Dans la cellule suivante, choisir un entier entre 100 et 1000 (il doit être personnel). Cet entier servira de graine au générateur de nombres aléatoire a conserver pour tous les exercices.\n",
        "\n"
      ],
      "metadata": {
        "id": "J3ga_6BNc5DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mySeed = 350"
      ],
      "metadata": {
        "id": "PrCTHM4od5UZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "TRWBLVpCWC06"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5RcggmAkJLV"
      },
      "source": [
        "\\\n",
        "\n",
        "**Exercice 1** *Une relation linéaire*\n",
        "\n",
        "La fonction *generate_dataset* fournit deux jeux de données (entraînement et test). Pour chaque jeu de données, la clef 'inputs' donne accès à un tableau numpy (numpy array) de prédicteurs empilés horizontalement : chaque ligne $i$ contient trois prédicteurs $x_i$, $y_i$ et $z_i$. La clef 'targets' renvoie le vecteur des cibles $t_i$. \\\n",
        "\n",
        "Les cibles sont liées aux prédicteurs par le modèle:\n",
        "$$ t = \\theta_0 + \\theta_1 x + \\theta_2 y + \\theta_3 z + \\epsilon$$ où $\\epsilon \\sim \\mathcal{N}(0,\\eta)$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils_exercices import generate_dataset, Dataset1\n",
        "train_set, test_set = generate_dataset(mySeed)"
      ],
      "metadata": {
        "id": "gEQmgTI8my8i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Par quelle méthode simple peut-on estimer les coefficients $\\theta_k$ ? La mettre en oeuvre avec la librairie python de votre choix."
      ],
      "metadata": {
        "id": "q5XZTrXNk12K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pour estimer les coefficients theta_k on peut utiliser la méthodes de la régression linéraire multiple\n",
        "# (moindres carrés ordinaires), qui consiste à trouver les coefficients qui minimisent la somme des carrés\n",
        "# des différences entre les valeurs prédites et les valeurs réelles.\n"
      ],
      "metadata": {
        "id": "HITtUqHhFMkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Extraire les prédicteurs (X) et les cibles (t)\n",
        "x  = train_set['inputs']\n",
        "t = train_set['targets']\n",
        "\n",
        "# Création du modèle d'entrainement : régression linéaire\n",
        "modele = LinearRegression()\n",
        "\n",
        "# Ajuster le modèle aux données d'entraînement\n",
        "modele.fit(x, t)\n",
        "\n",
        "print(\"Coefficients :\", modele.coef_)\n",
        "print(\"Interception :\", modele.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_y06qXMWDaB",
        "outputId": "17677412-2638-4e7a-c8dd-e0a89cd2d171"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients : [3.44301622 3.59207667 7.17464915]\n",
            "Interception : 17.43049692114625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans les cellules suivantes, on se propose d'estimer les $\\theta_k$ grâce à un réseau de neurones entraîné par SGD. Quelle architecture s'y prête ? Justifier en termes d'expressivité et de performances en généralisation puis la coder dans la cellule suivante."
      ],
      "metadata": {
        "id": "CH_Z5ZEIlQPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#L'architecture = c'est un réseau de neurone constitué d'une seule couche linéaire ce qui suifit car le\n",
        "# problème est linéaire. Cette couche calcule une combinaison linéaire pondérée des entrées en y ajoutant un\n",
        "# biais. Le réseau neuronal sera entrainé en utilisant la descente du gradient stochastique.\n",
        "\n",
        "# Expressivité  : t = theta0 + theta1.x + theta2.y + theta3.z\n",
        "# En terme de performance en généralisation : un modèle plus simple avec moins de paramètres est moins probable\n",
        "# d'ajouter des données d'entrainement ==> minimiser le surapprentissage.\n",
        "# En terme de performance : Ce qui conduit automatiquement a un entrainement plus rapide et plus efficace et\n",
        "# et une mailleure généralisation sur des données invisibles."
      ],
      "metadata": {
        "id": "x2QLFZwuXM-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset et dataloader :\n",
        "dataset = Dataset1(train_set['inputs'], train_set['targets'])\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)\n",
        "\n",
        "# A coder :\n",
        "import torch.nn as nn\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.linear = nn.Linear(3, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ],
      "metadata": {
        "id": "PPx543blnxdb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Entraîner cette architecture à la tâche de régression définie par les entrées et sorties du jeu d'entraînement (compléter la cellule ci-dessous)."
      ],
      "metadata": {
        "id": "g6BSTBitpGBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss, and optimizer\n",
        "mySimpleNet = SimpleNet()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(mySimpleNet.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_inputs, batch_targets in dataloader:\n",
        "\n",
        "       # Propagation avant\n",
        "        outputs = mySimpleNet(batch_inputs.float())\n",
        "        loss = criterion(outputs, batch_targets.float().unsqueeze(1))\n",
        "\n",
        "        # Rétropropagation et optimisation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Le valeur de la perte diminue bien à chaque époque, puis stagne à une valeur faible , ce qui est bon signe\n",
        "# sur l'aprentissage et que le modèle a bien appris à modéliser les données d'entraînement"
      ],
      "metadata": {
        "id": "Wjfa2Z4RoPO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45777165-7040-4bdf-89fb-e69c8689250d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/500], Loss: 335.7620\n",
            "Epoch [2/500], Loss: 175.2163\n",
            "Epoch [3/500], Loss: 116.4708\n",
            "Epoch [4/500], Loss: 75.0046\n",
            "Epoch [5/500], Loss: 41.5056\n",
            "Epoch [6/500], Loss: 33.0003\n",
            "Epoch [7/500], Loss: 16.3008\n",
            "Epoch [8/500], Loss: 13.5272\n",
            "Epoch [9/500], Loss: 10.2801\n",
            "Epoch [10/500], Loss: 7.2963\n",
            "Epoch [11/500], Loss: 7.2671\n",
            "Epoch [12/500], Loss: 5.8814\n",
            "Epoch [13/500], Loss: 5.1139\n",
            "Epoch [14/500], Loss: 5.1994\n",
            "Epoch [15/500], Loss: 4.8022\n",
            "Epoch [16/500], Loss: 4.5678\n",
            "Epoch [17/500], Loss: 4.5318\n",
            "Epoch [18/500], Loss: 4.0994\n",
            "Epoch [19/500], Loss: 4.5342\n",
            "Epoch [20/500], Loss: 4.9163\n",
            "Epoch [21/500], Loss: 3.8429\n",
            "Epoch [22/500], Loss: 3.9505\n",
            "Epoch [23/500], Loss: 4.2358\n",
            "Epoch [24/500], Loss: 4.1389\n",
            "Epoch [25/500], Loss: 3.4147\n",
            "Epoch [26/500], Loss: 4.6055\n",
            "Epoch [27/500], Loss: 3.9091\n",
            "Epoch [28/500], Loss: 4.1800\n",
            "Epoch [29/500], Loss: 3.9230\n",
            "Epoch [30/500], Loss: 4.4063\n",
            "Epoch [31/500], Loss: 4.4740\n",
            "Epoch [32/500], Loss: 4.1992\n",
            "Epoch [33/500], Loss: 4.5746\n",
            "Epoch [34/500], Loss: 3.6601\n",
            "Epoch [35/500], Loss: 3.7256\n",
            "Epoch [36/500], Loss: 3.7131\n",
            "Epoch [37/500], Loss: 4.6747\n",
            "Epoch [38/500], Loss: 4.0016\n",
            "Epoch [39/500], Loss: 4.7001\n",
            "Epoch [40/500], Loss: 4.4160\n",
            "Epoch [41/500], Loss: 4.2236\n",
            "Epoch [42/500], Loss: 4.6672\n",
            "Epoch [43/500], Loss: 5.1404\n",
            "Epoch [44/500], Loss: 3.7998\n",
            "Epoch [45/500], Loss: 4.3107\n",
            "Epoch [46/500], Loss: 4.7835\n",
            "Epoch [47/500], Loss: 3.9921\n",
            "Epoch [48/500], Loss: 4.4754\n",
            "Epoch [49/500], Loss: 2.9381\n",
            "Epoch [50/500], Loss: 3.4885\n",
            "Epoch [51/500], Loss: 3.7099\n",
            "Epoch [52/500], Loss: 3.9543\n",
            "Epoch [53/500], Loss: 3.6978\n",
            "Epoch [54/500], Loss: 4.1684\n",
            "Epoch [55/500], Loss: 4.7012\n",
            "Epoch [56/500], Loss: 3.4881\n",
            "Epoch [57/500], Loss: 4.6590\n",
            "Epoch [58/500], Loss: 4.1148\n",
            "Epoch [59/500], Loss: 4.8245\n",
            "Epoch [60/500], Loss: 4.1020\n",
            "Epoch [61/500], Loss: 4.2516\n",
            "Epoch [62/500], Loss: 3.2789\n",
            "Epoch [63/500], Loss: 3.8222\n",
            "Epoch [64/500], Loss: 3.7670\n",
            "Epoch [65/500], Loss: 4.6582\n",
            "Epoch [66/500], Loss: 3.4319\n",
            "Epoch [67/500], Loss: 3.3890\n",
            "Epoch [68/500], Loss: 4.2554\n",
            "Epoch [69/500], Loss: 3.2641\n",
            "Epoch [70/500], Loss: 3.1889\n",
            "Epoch [71/500], Loss: 4.3477\n",
            "Epoch [72/500], Loss: 3.4252\n",
            "Epoch [73/500], Loss: 4.5105\n",
            "Epoch [74/500], Loss: 3.7516\n",
            "Epoch [75/500], Loss: 4.4980\n",
            "Epoch [76/500], Loss: 4.2783\n",
            "Epoch [77/500], Loss: 4.6845\n",
            "Epoch [78/500], Loss: 4.4746\n",
            "Epoch [79/500], Loss: 4.5267\n",
            "Epoch [80/500], Loss: 4.4209\n",
            "Epoch [81/500], Loss: 3.6390\n",
            "Epoch [82/500], Loss: 5.3769\n",
            "Epoch [83/500], Loss: 4.1263\n",
            "Epoch [84/500], Loss: 3.2527\n",
            "Epoch [85/500], Loss: 3.7959\n",
            "Epoch [86/500], Loss: 4.2190\n",
            "Epoch [87/500], Loss: 4.5553\n",
            "Epoch [88/500], Loss: 3.8806\n",
            "Epoch [89/500], Loss: 4.3620\n",
            "Epoch [90/500], Loss: 4.2367\n",
            "Epoch [91/500], Loss: 3.7098\n",
            "Epoch [92/500], Loss: 3.6064\n",
            "Epoch [93/500], Loss: 4.7442\n",
            "Epoch [94/500], Loss: 4.7451\n",
            "Epoch [95/500], Loss: 3.6001\n",
            "Epoch [96/500], Loss: 4.2217\n",
            "Epoch [97/500], Loss: 4.8203\n",
            "Epoch [98/500], Loss: 3.8962\n",
            "Epoch [99/500], Loss: 4.0977\n",
            "Epoch [100/500], Loss: 4.2629\n",
            "Epoch [101/500], Loss: 4.1668\n",
            "Epoch [102/500], Loss: 3.6842\n",
            "Epoch [103/500], Loss: 3.4318\n",
            "Epoch [104/500], Loss: 3.8731\n",
            "Epoch [105/500], Loss: 3.7725\n",
            "Epoch [106/500], Loss: 4.3930\n",
            "Epoch [107/500], Loss: 4.4707\n",
            "Epoch [108/500], Loss: 3.4479\n",
            "Epoch [109/500], Loss: 3.9490\n",
            "Epoch [110/500], Loss: 4.9804\n",
            "Epoch [111/500], Loss: 4.2426\n",
            "Epoch [112/500], Loss: 4.8862\n",
            "Epoch [113/500], Loss: 4.1661\n",
            "Epoch [114/500], Loss: 3.4040\n",
            "Epoch [115/500], Loss: 4.3314\n",
            "Epoch [116/500], Loss: 3.9109\n",
            "Epoch [117/500], Loss: 3.7620\n",
            "Epoch [118/500], Loss: 3.3342\n",
            "Epoch [119/500], Loss: 3.6347\n",
            "Epoch [120/500], Loss: 5.1351\n",
            "Epoch [121/500], Loss: 3.8557\n",
            "Epoch [122/500], Loss: 5.0255\n",
            "Epoch [123/500], Loss: 3.4474\n",
            "Epoch [124/500], Loss: 3.8752\n",
            "Epoch [125/500], Loss: 3.7841\n",
            "Epoch [126/500], Loss: 4.1483\n",
            "Epoch [127/500], Loss: 4.1886\n",
            "Epoch [128/500], Loss: 4.7470\n",
            "Epoch [129/500], Loss: 3.2817\n",
            "Epoch [130/500], Loss: 4.4395\n",
            "Epoch [131/500], Loss: 5.1671\n",
            "Epoch [132/500], Loss: 5.6868\n",
            "Epoch [133/500], Loss: 3.6025\n",
            "Epoch [134/500], Loss: 4.4179\n",
            "Epoch [135/500], Loss: 3.4115\n",
            "Epoch [136/500], Loss: 4.6994\n",
            "Epoch [137/500], Loss: 4.0659\n",
            "Epoch [138/500], Loss: 4.2174\n",
            "Epoch [139/500], Loss: 4.9373\n",
            "Epoch [140/500], Loss: 3.1404\n",
            "Epoch [141/500], Loss: 5.0502\n",
            "Epoch [142/500], Loss: 3.4142\n",
            "Epoch [143/500], Loss: 4.2083\n",
            "Epoch [144/500], Loss: 4.4007\n",
            "Epoch [145/500], Loss: 3.4800\n",
            "Epoch [146/500], Loss: 3.2998\n",
            "Epoch [147/500], Loss: 3.8118\n",
            "Epoch [148/500], Loss: 4.2223\n",
            "Epoch [149/500], Loss: 3.9309\n",
            "Epoch [150/500], Loss: 3.9199\n",
            "Epoch [151/500], Loss: 2.8503\n",
            "Epoch [152/500], Loss: 3.3913\n",
            "Epoch [153/500], Loss: 3.7139\n",
            "Epoch [154/500], Loss: 4.2096\n",
            "Epoch [155/500], Loss: 4.5610\n",
            "Epoch [156/500], Loss: 4.2583\n",
            "Epoch [157/500], Loss: 4.7254\n",
            "Epoch [158/500], Loss: 3.7009\n",
            "Epoch [159/500], Loss: 4.6849\n",
            "Epoch [160/500], Loss: 4.6274\n",
            "Epoch [161/500], Loss: 3.9346\n",
            "Epoch [162/500], Loss: 3.5472\n",
            "Epoch [163/500], Loss: 4.7142\n",
            "Epoch [164/500], Loss: 3.6528\n",
            "Epoch [165/500], Loss: 4.4784\n",
            "Epoch [166/500], Loss: 3.7277\n",
            "Epoch [167/500], Loss: 3.9651\n",
            "Epoch [168/500], Loss: 4.9156\n",
            "Epoch [169/500], Loss: 3.5892\n",
            "Epoch [170/500], Loss: 4.6452\n",
            "Epoch [171/500], Loss: 4.2357\n",
            "Epoch [172/500], Loss: 3.8445\n",
            "Epoch [173/500], Loss: 4.8785\n",
            "Epoch [174/500], Loss: 3.5463\n",
            "Epoch [175/500], Loss: 4.6052\n",
            "Epoch [176/500], Loss: 4.0243\n",
            "Epoch [177/500], Loss: 4.1877\n",
            "Epoch [178/500], Loss: 4.3240\n",
            "Epoch [179/500], Loss: 4.1217\n",
            "Epoch [180/500], Loss: 3.4831\n",
            "Epoch [181/500], Loss: 5.2956\n",
            "Epoch [182/500], Loss: 3.8431\n",
            "Epoch [183/500], Loss: 4.4811\n",
            "Epoch [184/500], Loss: 4.6938\n",
            "Epoch [185/500], Loss: 4.6461\n",
            "Epoch [186/500], Loss: 3.2200\n",
            "Epoch [187/500], Loss: 4.4997\n",
            "Epoch [188/500], Loss: 4.1573\n",
            "Epoch [189/500], Loss: 3.5704\n",
            "Epoch [190/500], Loss: 3.3971\n",
            "Epoch [191/500], Loss: 3.6834\n",
            "Epoch [192/500], Loss: 3.8512\n",
            "Epoch [193/500], Loss: 3.7734\n",
            "Epoch [194/500], Loss: 3.7340\n",
            "Epoch [195/500], Loss: 3.7291\n",
            "Epoch [196/500], Loss: 4.4851\n",
            "Epoch [197/500], Loss: 4.5964\n",
            "Epoch [198/500], Loss: 4.2416\n",
            "Epoch [199/500], Loss: 4.6579\n",
            "Epoch [200/500], Loss: 4.1868\n",
            "Epoch [201/500], Loss: 5.2591\n",
            "Epoch [202/500], Loss: 4.4188\n",
            "Epoch [203/500], Loss: 5.6048\n",
            "Epoch [204/500], Loss: 3.9159\n",
            "Epoch [205/500], Loss: 4.0756\n",
            "Epoch [206/500], Loss: 3.6007\n",
            "Epoch [207/500], Loss: 3.8552\n",
            "Epoch [208/500], Loss: 4.3832\n",
            "Epoch [209/500], Loss: 4.1651\n",
            "Epoch [210/500], Loss: 4.0766\n",
            "Epoch [211/500], Loss: 4.7380\n",
            "Epoch [212/500], Loss: 3.3569\n",
            "Epoch [213/500], Loss: 4.1456\n",
            "Epoch [214/500], Loss: 3.7651\n",
            "Epoch [215/500], Loss: 4.5015\n",
            "Epoch [216/500], Loss: 4.4990\n",
            "Epoch [217/500], Loss: 4.5463\n",
            "Epoch [218/500], Loss: 4.8434\n",
            "Epoch [219/500], Loss: 3.8739\n",
            "Epoch [220/500], Loss: 3.6276\n",
            "Epoch [221/500], Loss: 4.7860\n",
            "Epoch [222/500], Loss: 3.9980\n",
            "Epoch [223/500], Loss: 4.2990\n",
            "Epoch [224/500], Loss: 4.9981\n",
            "Epoch [225/500], Loss: 4.2567\n",
            "Epoch [226/500], Loss: 3.9150\n",
            "Epoch [227/500], Loss: 4.0123\n",
            "Epoch [228/500], Loss: 4.1424\n",
            "Epoch [229/500], Loss: 4.2601\n",
            "Epoch [230/500], Loss: 4.1941\n",
            "Epoch [231/500], Loss: 3.4624\n",
            "Epoch [232/500], Loss: 4.7301\n",
            "Epoch [233/500], Loss: 4.5542\n",
            "Epoch [234/500], Loss: 4.3079\n",
            "Epoch [235/500], Loss: 3.8229\n",
            "Epoch [236/500], Loss: 4.2744\n",
            "Epoch [237/500], Loss: 2.9806\n",
            "Epoch [238/500], Loss: 3.7550\n",
            "Epoch [239/500], Loss: 3.8208\n",
            "Epoch [240/500], Loss: 4.1698\n",
            "Epoch [241/500], Loss: 4.4629\n",
            "Epoch [242/500], Loss: 4.5992\n",
            "Epoch [243/500], Loss: 3.9793\n",
            "Epoch [244/500], Loss: 3.7254\n",
            "Epoch [245/500], Loss: 4.0954\n",
            "Epoch [246/500], Loss: 3.7270\n",
            "Epoch [247/500], Loss: 4.2116\n",
            "Epoch [248/500], Loss: 5.5617\n",
            "Epoch [249/500], Loss: 3.2072\n",
            "Epoch [250/500], Loss: 4.3034\n",
            "Epoch [251/500], Loss: 6.2856\n",
            "Epoch [252/500], Loss: 3.6253\n",
            "Epoch [253/500], Loss: 4.3972\n",
            "Epoch [254/500], Loss: 4.9096\n",
            "Epoch [255/500], Loss: 3.6223\n",
            "Epoch [256/500], Loss: 3.7944\n",
            "Epoch [257/500], Loss: 4.7065\n",
            "Epoch [258/500], Loss: 4.1044\n",
            "Epoch [259/500], Loss: 4.6141\n",
            "Epoch [260/500], Loss: 3.8740\n",
            "Epoch [261/500], Loss: 4.4255\n",
            "Epoch [262/500], Loss: 4.1332\n",
            "Epoch [263/500], Loss: 3.8285\n",
            "Epoch [264/500], Loss: 3.9957\n",
            "Epoch [265/500], Loss: 4.0778\n",
            "Epoch [266/500], Loss: 4.1775\n",
            "Epoch [267/500], Loss: 4.5303\n",
            "Epoch [268/500], Loss: 4.3139\n",
            "Epoch [269/500], Loss: 3.3243\n",
            "Epoch [270/500], Loss: 4.3727\n",
            "Epoch [271/500], Loss: 3.9504\n",
            "Epoch [272/500], Loss: 4.2419\n",
            "Epoch [273/500], Loss: 5.0161\n",
            "Epoch [274/500], Loss: 4.1784\n",
            "Epoch [275/500], Loss: 4.1683\n",
            "Epoch [276/500], Loss: 4.2687\n",
            "Epoch [277/500], Loss: 3.8937\n",
            "Epoch [278/500], Loss: 3.8004\n",
            "Epoch [279/500], Loss: 4.7239\n",
            "Epoch [280/500], Loss: 3.7727\n",
            "Epoch [281/500], Loss: 4.0807\n",
            "Epoch [282/500], Loss: 3.9337\n",
            "Epoch [283/500], Loss: 3.4941\n",
            "Epoch [284/500], Loss: 3.8954\n",
            "Epoch [285/500], Loss: 2.8790\n",
            "Epoch [286/500], Loss: 4.0569\n",
            "Epoch [287/500], Loss: 3.6134\n",
            "Epoch [288/500], Loss: 3.7878\n",
            "Epoch [289/500], Loss: 4.3945\n",
            "Epoch [290/500], Loss: 5.1842\n",
            "Epoch [291/500], Loss: 4.0172\n",
            "Epoch [292/500], Loss: 4.1296\n",
            "Epoch [293/500], Loss: 4.9683\n",
            "Epoch [294/500], Loss: 4.1733\n",
            "Epoch [295/500], Loss: 4.3833\n",
            "Epoch [296/500], Loss: 4.3998\n",
            "Epoch [297/500], Loss: 3.6199\n",
            "Epoch [298/500], Loss: 4.0700\n",
            "Epoch [299/500], Loss: 4.1050\n",
            "Epoch [300/500], Loss: 3.5737\n",
            "Epoch [301/500], Loss: 4.6004\n",
            "Epoch [302/500], Loss: 3.6171\n",
            "Epoch [303/500], Loss: 3.9918\n",
            "Epoch [304/500], Loss: 4.2022\n",
            "Epoch [305/500], Loss: 4.3582\n",
            "Epoch [306/500], Loss: 3.7877\n",
            "Epoch [307/500], Loss: 4.5788\n",
            "Epoch [308/500], Loss: 3.9590\n",
            "Epoch [309/500], Loss: 4.2937\n",
            "Epoch [310/500], Loss: 4.6175\n",
            "Epoch [311/500], Loss: 3.8266\n",
            "Epoch [312/500], Loss: 4.0853\n",
            "Epoch [313/500], Loss: 3.7688\n",
            "Epoch [314/500], Loss: 3.2130\n",
            "Epoch [315/500], Loss: 4.3686\n",
            "Epoch [316/500], Loss: 3.9193\n",
            "Epoch [317/500], Loss: 4.9478\n",
            "Epoch [318/500], Loss: 3.1995\n",
            "Epoch [319/500], Loss: 3.7772\n",
            "Epoch [320/500], Loss: 3.8241\n",
            "Epoch [321/500], Loss: 3.9352\n",
            "Epoch [322/500], Loss: 3.8796\n",
            "Epoch [323/500], Loss: 4.6199\n",
            "Epoch [324/500], Loss: 4.3309\n",
            "Epoch [325/500], Loss: 5.5113\n",
            "Epoch [326/500], Loss: 4.3730\n",
            "Epoch [327/500], Loss: 3.5065\n",
            "Epoch [328/500], Loss: 4.5048\n",
            "Epoch [329/500], Loss: 4.7397\n",
            "Epoch [330/500], Loss: 3.8834\n",
            "Epoch [331/500], Loss: 4.4878\n",
            "Epoch [332/500], Loss: 3.2986\n",
            "Epoch [333/500], Loss: 4.9472\n",
            "Epoch [334/500], Loss: 3.3635\n",
            "Epoch [335/500], Loss: 4.2802\n",
            "Epoch [336/500], Loss: 4.6449\n",
            "Epoch [337/500], Loss: 3.7151\n",
            "Epoch [338/500], Loss: 4.0443\n",
            "Epoch [339/500], Loss: 4.4264\n",
            "Epoch [340/500], Loss: 3.4930\n",
            "Epoch [341/500], Loss: 3.5827\n",
            "Epoch [342/500], Loss: 4.7808\n",
            "Epoch [343/500], Loss: 4.4229\n",
            "Epoch [344/500], Loss: 4.7553\n",
            "Epoch [345/500], Loss: 4.0819\n",
            "Epoch [346/500], Loss: 4.1573\n",
            "Epoch [347/500], Loss: 3.6808\n",
            "Epoch [348/500], Loss: 3.6959\n",
            "Epoch [349/500], Loss: 4.2712\n",
            "Epoch [350/500], Loss: 3.7987\n",
            "Epoch [351/500], Loss: 3.9508\n",
            "Epoch [352/500], Loss: 4.5245\n",
            "Epoch [353/500], Loss: 4.2715\n",
            "Epoch [354/500], Loss: 3.3961\n",
            "Epoch [355/500], Loss: 4.8129\n",
            "Epoch [356/500], Loss: 4.8168\n",
            "Epoch [357/500], Loss: 4.6680\n",
            "Epoch [358/500], Loss: 3.4254\n",
            "Epoch [359/500], Loss: 3.3419\n",
            "Epoch [360/500], Loss: 4.0558\n",
            "Epoch [361/500], Loss: 4.4658\n",
            "Epoch [362/500], Loss: 4.2577\n",
            "Epoch [363/500], Loss: 4.2357\n",
            "Epoch [364/500], Loss: 3.8022\n",
            "Epoch [365/500], Loss: 3.5763\n",
            "Epoch [366/500], Loss: 4.4728\n",
            "Epoch [367/500], Loss: 3.8879\n",
            "Epoch [368/500], Loss: 4.5008\n",
            "Epoch [369/500], Loss: 3.4593\n",
            "Epoch [370/500], Loss: 4.1652\n",
            "Epoch [371/500], Loss: 3.7696\n",
            "Epoch [372/500], Loss: 3.1664\n",
            "Epoch [373/500], Loss: 4.3722\n",
            "Epoch [374/500], Loss: 4.1366\n",
            "Epoch [375/500], Loss: 3.8978\n",
            "Epoch [376/500], Loss: 4.6005\n",
            "Epoch [377/500], Loss: 4.2474\n",
            "Epoch [378/500], Loss: 4.5736\n",
            "Epoch [379/500], Loss: 3.9885\n",
            "Epoch [380/500], Loss: 3.7911\n",
            "Epoch [381/500], Loss: 4.3179\n",
            "Epoch [382/500], Loss: 3.1984\n",
            "Epoch [383/500], Loss: 4.1224\n",
            "Epoch [384/500], Loss: 4.3639\n",
            "Epoch [385/500], Loss: 4.4448\n",
            "Epoch [386/500], Loss: 4.3116\n",
            "Epoch [387/500], Loss: 4.0787\n",
            "Epoch [388/500], Loss: 4.1044\n",
            "Epoch [389/500], Loss: 4.6452\n",
            "Epoch [390/500], Loss: 5.3032\n",
            "Epoch [391/500], Loss: 3.8441\n",
            "Epoch [392/500], Loss: 3.4327\n",
            "Epoch [393/500], Loss: 4.1183\n",
            "Epoch [394/500], Loss: 4.8243\n",
            "Epoch [395/500], Loss: 4.2417\n",
            "Epoch [396/500], Loss: 3.5661\n",
            "Epoch [397/500], Loss: 4.8216\n",
            "Epoch [398/500], Loss: 3.9423\n",
            "Epoch [399/500], Loss: 4.2193\n",
            "Epoch [400/500], Loss: 3.6020\n",
            "Epoch [401/500], Loss: 3.2915\n",
            "Epoch [402/500], Loss: 3.8015\n",
            "Epoch [403/500], Loss: 3.6049\n",
            "Epoch [404/500], Loss: 3.6110\n",
            "Epoch [405/500], Loss: 3.5417\n",
            "Epoch [406/500], Loss: 3.8483\n",
            "Epoch [407/500], Loss: 4.5221\n",
            "Epoch [408/500], Loss: 3.6468\n",
            "Epoch [409/500], Loss: 5.1896\n",
            "Epoch [410/500], Loss: 3.8232\n",
            "Epoch [411/500], Loss: 3.7911\n",
            "Epoch [412/500], Loss: 4.4968\n",
            "Epoch [413/500], Loss: 3.6222\n",
            "Epoch [414/500], Loss: 3.5397\n",
            "Epoch [415/500], Loss: 4.3233\n",
            "Epoch [416/500], Loss: 3.9181\n",
            "Epoch [417/500], Loss: 4.6428\n",
            "Epoch [418/500], Loss: 4.8652\n",
            "Epoch [419/500], Loss: 5.2894\n",
            "Epoch [420/500], Loss: 3.3529\n",
            "Epoch [421/500], Loss: 3.9555\n",
            "Epoch [422/500], Loss: 3.9947\n",
            "Epoch [423/500], Loss: 3.7598\n",
            "Epoch [424/500], Loss: 3.6861\n",
            "Epoch [425/500], Loss: 4.1207\n",
            "Epoch [426/500], Loss: 4.5936\n",
            "Epoch [427/500], Loss: 2.9026\n",
            "Epoch [428/500], Loss: 4.1801\n",
            "Epoch [429/500], Loss: 3.8709\n",
            "Epoch [430/500], Loss: 3.5615\n",
            "Epoch [431/500], Loss: 3.6253\n",
            "Epoch [432/500], Loss: 5.0531\n",
            "Epoch [433/500], Loss: 4.0629\n",
            "Epoch [434/500], Loss: 4.6165\n",
            "Epoch [435/500], Loss: 3.2772\n",
            "Epoch [436/500], Loss: 4.4714\n",
            "Epoch [437/500], Loss: 3.7634\n",
            "Epoch [438/500], Loss: 3.5217\n",
            "Epoch [439/500], Loss: 3.2851\n",
            "Epoch [440/500], Loss: 3.7056\n",
            "Epoch [441/500], Loss: 4.4663\n",
            "Epoch [442/500], Loss: 4.3728\n",
            "Epoch [443/500], Loss: 3.9726\n",
            "Epoch [444/500], Loss: 4.7779\n",
            "Epoch [445/500], Loss: 4.1428\n",
            "Epoch [446/500], Loss: 4.3895\n",
            "Epoch [447/500], Loss: 4.2212\n",
            "Epoch [448/500], Loss: 4.6826\n",
            "Epoch [449/500], Loss: 4.5938\n",
            "Epoch [450/500], Loss: 4.0847\n",
            "Epoch [451/500], Loss: 5.2144\n",
            "Epoch [452/500], Loss: 3.5444\n",
            "Epoch [453/500], Loss: 3.6939\n",
            "Epoch [454/500], Loss: 3.0672\n",
            "Epoch [455/500], Loss: 4.3897\n",
            "Epoch [456/500], Loss: 4.8656\n",
            "Epoch [457/500], Loss: 3.9235\n",
            "Epoch [458/500], Loss: 4.2894\n",
            "Epoch [459/500], Loss: 4.2448\n",
            "Epoch [460/500], Loss: 4.4183\n",
            "Epoch [461/500], Loss: 3.3090\n",
            "Epoch [462/500], Loss: 4.7186\n",
            "Epoch [463/500], Loss: 4.1722\n",
            "Epoch [464/500], Loss: 2.9492\n",
            "Epoch [465/500], Loss: 4.6618\n",
            "Epoch [466/500], Loss: 4.2459\n",
            "Epoch [467/500], Loss: 2.6973\n",
            "Epoch [468/500], Loss: 4.1590\n",
            "Epoch [469/500], Loss: 4.0240\n",
            "Epoch [470/500], Loss: 4.5357\n",
            "Epoch [471/500], Loss: 3.5612\n",
            "Epoch [472/500], Loss: 3.8355\n",
            "Epoch [473/500], Loss: 4.6513\n",
            "Epoch [474/500], Loss: 4.3252\n",
            "Epoch [475/500], Loss: 4.4536\n",
            "Epoch [476/500], Loss: 3.1936\n",
            "Epoch [477/500], Loss: 3.5321\n",
            "Epoch [478/500], Loss: 4.6417\n",
            "Epoch [479/500], Loss: 4.5031\n",
            "Epoch [480/500], Loss: 3.1584\n",
            "Epoch [481/500], Loss: 3.9636\n",
            "Epoch [482/500], Loss: 4.0394\n",
            "Epoch [483/500], Loss: 3.8684\n",
            "Epoch [484/500], Loss: 3.9750\n",
            "Epoch [485/500], Loss: 3.7092\n",
            "Epoch [486/500], Loss: 4.9335\n",
            "Epoch [487/500], Loss: 4.5188\n",
            "Epoch [488/500], Loss: 4.1266\n",
            "Epoch [489/500], Loss: 3.8358\n",
            "Epoch [490/500], Loss: 4.0493\n",
            "Epoch [491/500], Loss: 4.7798\n",
            "Epoch [492/500], Loss: 4.2918\n",
            "Epoch [493/500], Loss: 3.9412\n",
            "Epoch [494/500], Loss: 3.0019\n",
            "Epoch [495/500], Loss: 4.5080\n",
            "Epoch [496/500], Loss: 4.5348\n",
            "Epoch [497/500], Loss: 4.3249\n",
            "Epoch [498/500], Loss: 4.1006\n",
            "Epoch [499/500], Loss: 4.7517\n",
            "Epoch [500/500], Loss: 3.9854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Où sont alors stockées les estimations des  $\\theta_k$ ? Les extraire du réseau *mySimpleNet* dans la cellule suivante."
      ],
      "metadata": {
        "id": "OZwKogEEp2Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EjgWp1y1rseb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** Tester ces estimations sur le jeu de test et comparer avec celles de la question 1. Commentez."
      ],
      "metadata": {
        "id": "pEB-V-oOrJED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "VvV2jIrBNtzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 2** *Champ réceptif et prédiction causale*"
      ],
      "metadata": {
        "id": "CpRvXCaAtsIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le réseau défini dans la cellule suivante est utilisé pour faire le lien entre les valeurs $(x_{t' \\leq t})$ d'une série temporelle d'entrée et la valeur présente $y_t$ d'une série temporelle cible."
      ],
      "metadata": {
        "id": "8JG9wTfK5TBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from utils_exercices import Outconv, Up_causal, Down_causal\n",
        "\n",
        "class Double_conv_causal(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2, with causal convolutions that preserve input size'''\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, dilation=1):\n",
        "        super(Double_conv_causal, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation = dilation\n",
        "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn1 = nn.BatchNorm1d(out_ch)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm1d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class causalFCN(nn.Module):\n",
        "    def __init__(self, dilation=1):\n",
        "        super(causalFCN, self).__init__()\n",
        "        size = 64\n",
        "        n_channels = 1\n",
        "        n_classes = 1\n",
        "        self.inc = Double_conv_causal(n_channels, size)\n",
        "        self.down1 = Down_causal(size, 2*size)\n",
        "        self.down2 = Down_causal(2*size, 4*size)\n",
        "        self.down3 = Down_causal(4*size, 8*size, pooling_kernel_size=5, pooling_stride=5)\n",
        "        self.down4 = Down_causal(8*size, 4*size, pooling=False, dilation=2)\n",
        "        self.up2 = Up_causal(4*size, 2*size, kernel_size=5, stride=5)\n",
        "        self.up3 = Up_causal(2*size, size)\n",
        "        self.up4 = Up_causal(size, size)\n",
        "        self.outc = Outconv(size, n_classes)\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up2(x5, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return x\n",
        "\n",
        "# Exemple d'utilisation\n",
        "model = causalFCN()\n",
        "# Série temporelle d'entrée (x_t):\n",
        "input_tensor1 = torch.rand(1, 1, 10000)\n",
        "# Série temporelle en sortie f(x_t):\n",
        "output = model(input_tensor1)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "fIbU1EJT1MM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** De quel type de réseau de neurones s'agit-il ? Combien de paramètres la couche self.Down1 compte-t-elle (à faire à la main) ?\n",
        "Combien de paramètres le réseau entier compte-t-il (avec un peu de code) ?"
      ],
      "metadata": {
        "id": "-mNnsYU-7R7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nb de paramètres dans self.Down1: (calcul \"à la main\")\n",
        "\n",
        "# Nb de paramètres au total:"
      ],
      "metadata": {
        "id": "qlYxUf6U9vH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Par quels mécanismes la taille du vecteur d'entrée est-elle réduite ? Comment est-elle restituée dans la deuxième partie du réseau ?"
      ],
      "metadata": {
        "id": "I4D46A0-8LaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Par quels mécanismes le champ réceptif est-il augmenté ? Préciser par un calcul la taille du champ réceptif en sortie de *self.inc*."
      ],
      "metadata": {
        "id": "SVNeFnm88yV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Par un bout de code, déterminer empiriquement la taille du champ réceptif associé à la composante $y_{5000}$ du vecteur de sortie. (Indice: considérer les sorties associées à deux inputs qui ne diffèrent que par une composante...)"
      ],
      "metadata": {
        "id": "TVVcBPuA9EP0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "69WMWCSZAg5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** $y_{5000}$ dépend-elle des composantes $x_{t, \\space t > 5000}$ ? Justifier de manière empirique puis préciser la partie du code de Double_conv_causal qui garantit cette propriété de \"causalité\" en justifiant.  \n",
        "\n"
      ],
      "metadata": {
        "id": "gZ37skwm-Vpv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PeooRYE-ATGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "qV52tusgNn6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "Exercice 3: \"Ranknet loss\""
      ],
      "metadata": {
        "id": "bm-sRzmfqc2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un [article récent](https://https://arxiv.org/abs/2403.14144) revient sur les progrès en matière de learning to rank. En voilà un extrait :"
      ],
      "metadata": {
        "id": "Wl8wUjsSM57D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/nanopiero/exam_2025/refs/heads/main/utils/png_exercice3.PNG?token=GHSAT0AAAAAAC427DACOPGNDNN6UDOLVLLAZ4BB2JQ\" alt=\"extrait d'un article\" width=\"800\">"
      ],
      "metadata": {
        "id": "SDZUXMlSDpoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Qu'est-ce que les auteurs appellent \"positive samples\" et \"negative samples\" ? Donner un exemple."
      ],
      "metadata": {
        "id": "9NzV1PbMNyuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans l'expression de $\\mathcal{L}_{RankNet}$, d'où proviennent les $z_i$ ? Que représentent-ils ?  "
      ],
      "metadata": {
        "id": "yIKQ5Eo9OnPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Pourquoi cette expression conduit-elle à ce que, après apprentissage, \"the estimated\n",
        "value of positive samples is greater than that of negative samples\n",
        "for each pair of positive/negative samples\" ?"
      ],
      "metadata": {
        "id": "r74fWiyvPb7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Dans le cadre d'une approche par deep learning, quels termes utilise-t-on pour qualifier les réseaux de neurones exploités et la modalité suivant laquelle ils sont entraînés ?"
      ],
      "metadata": {
        "id": "pk1EIi_VVi3R"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}