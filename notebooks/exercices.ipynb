{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CACBFsndOCo"
      },
      "source": [
        "# Exercices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Préliminaires**: Clone de votre repo et imports"
      ],
      "metadata": {
        "id": "hfkMtaHleKAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/cutiepie1011/exam_2025.git\n",
        "! cp exam_2025/utils/utils_exercices.py .\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "xiD_cI-geJjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46214b6b-6a8d-41a6-aef3-fd426c64e764"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'exam_2025' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clef personnelle pour la partie théorique**\n",
        "\n",
        "Dans la cellule suivante, choisir un entier entre 100 et 1000 (il doit être personnel). Cet entier servira de graine au générateur de nombres aléatoire a conserver pour tous les exercices.\n",
        "\n"
      ],
      "metadata": {
        "id": "J3ga_6BNc5DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mySeed = 350"
      ],
      "metadata": {
        "id": "PrCTHM4od5UZ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "TRWBLVpCWC06"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5RcggmAkJLV"
      },
      "source": [
        "\\\n",
        "\n",
        "**Exercice 1** *Une relation linéaire*\n",
        "\n",
        "La fonction *generate_dataset* fournit deux jeux de données (entraînement et test). Pour chaque jeu de données, la clef 'inputs' donne accès à un tableau numpy (numpy array) de prédicteurs empilés horizontalement : chaque ligne $i$ contient trois prédicteurs $x_i$, $y_i$ et $z_i$. La clef 'targets' renvoie le vecteur des cibles $t_i$. \\\n",
        "\n",
        "Les cibles sont liées aux prédicteurs par le modèle:\n",
        "$$ t = \\theta_0 + \\theta_1 x + \\theta_2 y + \\theta_3 z + \\epsilon$$ où $\\epsilon \\sim \\mathcal{N}(0,\\eta)$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils_exercices import generate_dataset, Dataset1\n",
        "train_set, test_set = generate_dataset(mySeed)"
      ],
      "metadata": {
        "id": "gEQmgTI8my8i"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Par quelle méthode simple peut-on estimer les coefficients $\\theta_k$ ? La mettre en oeuvre avec la librairie python de votre choix."
      ],
      "metadata": {
        "id": "q5XZTrXNk12K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pour estimer les coefficients theta_k on peut utiliser la méthodes de la régression linéraire multiple\n",
        "# (moindres carrés ordinaires), qui consiste à trouver les coefficients qui minimisent la somme des carrés\n",
        "# des différences entre les valeurs prédites et les valeurs réelles.\n"
      ],
      "metadata": {
        "id": "HITtUqHhFMkn"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Extraire les prédicteurs (X) et les cibles (t)\n",
        "x  = train_set['inputs']\n",
        "t = train_set['targets']\n",
        "\n",
        "# Création du modèle d'entrainement : régression linéaire\n",
        "modele = LinearRegression()\n",
        "\n",
        "# Ajuster le modèle aux données d'entraînement\n",
        "modele.fit(x, t)\n",
        "\n",
        "print(\"Coefficients :\", modele.coef_)\n",
        "print(\"Interception :\", modele.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_y06qXMWDaB",
        "outputId": "8057adbb-3fc9-449a-8e35-06ef06719b36"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients : [3.44301622 3.59207667 7.17464915]\n",
            "Interception : 17.43049692114625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans les cellules suivantes, on se propose d'estimer les $\\theta_k$ grâce à un réseau de neurones entraîné par SGD. Quelle architecture s'y prête ? Justifier en termes d'expressivité et de performances en généralisation puis la coder dans la cellule suivante."
      ],
      "metadata": {
        "id": "CH_Z5ZEIlQPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#L'architecture = c'est un réseau de neurone constitué d'une seule couche linéaire ce qui suifit car le\n",
        "# problème est linéaire. Cette couche calcule une combinaison linéaire pondérée des entrées en y ajoutant un\n",
        "# biais. Le réseau neuronal sera entrainé en utilisant la descente du gradient stochastique.\n",
        "\n",
        "# Expressivité  : t = theta0 + theta1.x + theta2.y + theta3.z\n",
        "# En terme de performance en généralisation : un modèle plus simple avec moins de paramètres est moins probable\n",
        "# d'ajouter des données d'entrainement ==> minimiser le surapprentissage.\n",
        "# En terme de performance : Ce qui conduit automatiquement a un entrainement plus rapide et plus efficace et\n",
        "# et une mailleure généralisation sur des données invisibles."
      ],
      "metadata": {
        "id": "x2QLFZwuXM-Y"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset et dataloader :\n",
        "dataset = Dataset1(train_set['inputs'], train_set['targets'])\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)\n",
        "\n",
        "# A coder :\n",
        "import torch.nn as nn\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.linear = nn.Linear(3, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ],
      "metadata": {
        "id": "PPx543blnxdb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Entraîner cette architecture à la tâche de régression définie par les entrées et sorties du jeu d'entraînement (compléter la cellule ci-dessous)."
      ],
      "metadata": {
        "id": "g6BSTBitpGBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss, and optimizer\n",
        "mySimpleNet = SimpleNet()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(mySimpleNet.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_inputs, batch_targets in dataloader:\n",
        "\n",
        "       # Propagation avant\n",
        "        outputs = mySimpleNet(batch_inputs.float())\n",
        "        loss = criterion(outputs, batch_targets.float().unsqueeze(1))\n",
        "\n",
        "        # Rétropropagation et optimisation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Le valeur de la perte diminue bien à chaque époque, puis stagne à une valeur faible , ce qui est bon signe\n",
        "# sur l'aprentissage et que le modèle a bien appris à modéliser les données d'entraînement"
      ],
      "metadata": {
        "id": "Wjfa2Z4RoPO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17fe005c-9b4e-47a3-9533-b2eb452454fa"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/500], Loss: 285.1299\n",
            "Epoch [2/500], Loss: 168.1904\n",
            "Epoch [3/500], Loss: 109.7982\n",
            "Epoch [4/500], Loss: 61.0179\n",
            "Epoch [5/500], Loss: 41.4430\n",
            "Epoch [6/500], Loss: 30.5691\n",
            "Epoch [7/500], Loss: 16.5878\n",
            "Epoch [8/500], Loss: 15.6148\n",
            "Epoch [9/500], Loss: 10.5494\n",
            "Epoch [10/500], Loss: 5.9747\n",
            "Epoch [11/500], Loss: 5.3427\n",
            "Epoch [12/500], Loss: 5.1720\n",
            "Epoch [13/500], Loss: 3.5665\n",
            "Epoch [14/500], Loss: 4.5485\n",
            "Epoch [15/500], Loss: 4.4740\n",
            "Epoch [16/500], Loss: 5.1485\n",
            "Epoch [17/500], Loss: 4.8590\n",
            "Epoch [18/500], Loss: 4.5817\n",
            "Epoch [19/500], Loss: 5.0041\n",
            "Epoch [20/500], Loss: 5.6978\n",
            "Epoch [21/500], Loss: 4.2130\n",
            "Epoch [22/500], Loss: 4.8267\n",
            "Epoch [23/500], Loss: 4.0846\n",
            "Epoch [24/500], Loss: 4.0418\n",
            "Epoch [25/500], Loss: 4.0144\n",
            "Epoch [26/500], Loss: 4.0567\n",
            "Epoch [27/500], Loss: 4.1564\n",
            "Epoch [28/500], Loss: 4.2055\n",
            "Epoch [29/500], Loss: 3.6420\n",
            "Epoch [30/500], Loss: 3.3631\n",
            "Epoch [31/500], Loss: 4.2401\n",
            "Epoch [32/500], Loss: 3.8739\n",
            "Epoch [33/500], Loss: 3.8838\n",
            "Epoch [34/500], Loss: 4.0896\n",
            "Epoch [35/500], Loss: 3.5202\n",
            "Epoch [36/500], Loss: 3.5957\n",
            "Epoch [37/500], Loss: 4.0351\n",
            "Epoch [38/500], Loss: 4.1349\n",
            "Epoch [39/500], Loss: 3.6396\n",
            "Epoch [40/500], Loss: 4.1376\n",
            "Epoch [41/500], Loss: 4.9184\n",
            "Epoch [42/500], Loss: 3.8694\n",
            "Epoch [43/500], Loss: 3.6270\n",
            "Epoch [44/500], Loss: 4.2506\n",
            "Epoch [45/500], Loss: 4.2306\n",
            "Epoch [46/500], Loss: 4.2756\n",
            "Epoch [47/500], Loss: 4.1710\n",
            "Epoch [48/500], Loss: 4.1319\n",
            "Epoch [49/500], Loss: 4.0425\n",
            "Epoch [50/500], Loss: 3.5996\n",
            "Epoch [51/500], Loss: 4.3933\n",
            "Epoch [52/500], Loss: 3.7213\n",
            "Epoch [53/500], Loss: 3.8347\n",
            "Epoch [54/500], Loss: 2.7520\n",
            "Epoch [55/500], Loss: 4.3848\n",
            "Epoch [56/500], Loss: 4.6965\n",
            "Epoch [57/500], Loss: 4.6440\n",
            "Epoch [58/500], Loss: 4.4388\n",
            "Epoch [59/500], Loss: 4.7457\n",
            "Epoch [60/500], Loss: 3.1994\n",
            "Epoch [61/500], Loss: 3.8240\n",
            "Epoch [62/500], Loss: 3.5806\n",
            "Epoch [63/500], Loss: 3.3339\n",
            "Epoch [64/500], Loss: 4.4197\n",
            "Epoch [65/500], Loss: 4.4275\n",
            "Epoch [66/500], Loss: 4.2546\n",
            "Epoch [67/500], Loss: 4.7536\n",
            "Epoch [68/500], Loss: 4.4752\n",
            "Epoch [69/500], Loss: 4.2441\n",
            "Epoch [70/500], Loss: 3.7843\n",
            "Epoch [71/500], Loss: 4.4362\n",
            "Epoch [72/500], Loss: 4.4772\n",
            "Epoch [73/500], Loss: 3.0864\n",
            "Epoch [74/500], Loss: 3.8962\n",
            "Epoch [75/500], Loss: 3.1410\n",
            "Epoch [76/500], Loss: 4.4774\n",
            "Epoch [77/500], Loss: 3.7536\n",
            "Epoch [78/500], Loss: 3.4979\n",
            "Epoch [79/500], Loss: 3.2728\n",
            "Epoch [80/500], Loss: 3.9639\n",
            "Epoch [81/500], Loss: 3.9478\n",
            "Epoch [82/500], Loss: 3.6543\n",
            "Epoch [83/500], Loss: 5.0004\n",
            "Epoch [84/500], Loss: 4.2382\n",
            "Epoch [85/500], Loss: 4.0002\n",
            "Epoch [86/500], Loss: 3.9663\n",
            "Epoch [87/500], Loss: 4.1076\n",
            "Epoch [88/500], Loss: 4.3846\n",
            "Epoch [89/500], Loss: 4.1323\n",
            "Epoch [90/500], Loss: 4.7035\n",
            "Epoch [91/500], Loss: 3.1327\n",
            "Epoch [92/500], Loss: 3.9796\n",
            "Epoch [93/500], Loss: 4.8265\n",
            "Epoch [94/500], Loss: 4.5352\n",
            "Epoch [95/500], Loss: 3.6179\n",
            "Epoch [96/500], Loss: 4.0000\n",
            "Epoch [97/500], Loss: 4.2325\n",
            "Epoch [98/500], Loss: 3.6587\n",
            "Epoch [99/500], Loss: 3.6168\n",
            "Epoch [100/500], Loss: 4.5469\n",
            "Epoch [101/500], Loss: 3.0601\n",
            "Epoch [102/500], Loss: 5.5806\n",
            "Epoch [103/500], Loss: 4.2705\n",
            "Epoch [104/500], Loss: 5.0573\n",
            "Epoch [105/500], Loss: 3.5687\n",
            "Epoch [106/500], Loss: 3.7880\n",
            "Epoch [107/500], Loss: 4.2474\n",
            "Epoch [108/500], Loss: 3.9421\n",
            "Epoch [109/500], Loss: 3.7752\n",
            "Epoch [110/500], Loss: 3.9945\n",
            "Epoch [111/500], Loss: 3.2617\n",
            "Epoch [112/500], Loss: 4.2110\n",
            "Epoch [113/500], Loss: 3.3098\n",
            "Epoch [114/500], Loss: 4.9575\n",
            "Epoch [115/500], Loss: 4.0771\n",
            "Epoch [116/500], Loss: 3.0712\n",
            "Epoch [117/500], Loss: 3.6602\n",
            "Epoch [118/500], Loss: 3.8448\n",
            "Epoch [119/500], Loss: 3.0246\n",
            "Epoch [120/500], Loss: 4.3560\n",
            "Epoch [121/500], Loss: 3.8678\n",
            "Epoch [122/500], Loss: 3.9975\n",
            "Epoch [123/500], Loss: 4.8882\n",
            "Epoch [124/500], Loss: 3.8820\n",
            "Epoch [125/500], Loss: 3.8402\n",
            "Epoch [126/500], Loss: 4.2561\n",
            "Epoch [127/500], Loss: 4.1244\n",
            "Epoch [128/500], Loss: 3.9323\n",
            "Epoch [129/500], Loss: 4.9580\n",
            "Epoch [130/500], Loss: 3.8482\n",
            "Epoch [131/500], Loss: 4.4053\n",
            "Epoch [132/500], Loss: 4.2705\n",
            "Epoch [133/500], Loss: 4.1445\n",
            "Epoch [134/500], Loss: 3.5901\n",
            "Epoch [135/500], Loss: 4.0428\n",
            "Epoch [136/500], Loss: 4.1460\n",
            "Epoch [137/500], Loss: 3.7434\n",
            "Epoch [138/500], Loss: 4.5627\n",
            "Epoch [139/500], Loss: 3.9114\n",
            "Epoch [140/500], Loss: 4.1199\n",
            "Epoch [141/500], Loss: 4.2913\n",
            "Epoch [142/500], Loss: 4.4875\n",
            "Epoch [143/500], Loss: 3.0107\n",
            "Epoch [144/500], Loss: 4.7540\n",
            "Epoch [145/500], Loss: 4.1656\n",
            "Epoch [146/500], Loss: 4.6034\n",
            "Epoch [147/500], Loss: 3.9265\n",
            "Epoch [148/500], Loss: 4.2380\n",
            "Epoch [149/500], Loss: 3.9422\n",
            "Epoch [150/500], Loss: 4.7560\n",
            "Epoch [151/500], Loss: 3.0937\n",
            "Epoch [152/500], Loss: 4.3908\n",
            "Epoch [153/500], Loss: 3.6183\n",
            "Epoch [154/500], Loss: 3.8379\n",
            "Epoch [155/500], Loss: 3.5574\n",
            "Epoch [156/500], Loss: 4.3582\n",
            "Epoch [157/500], Loss: 4.4563\n",
            "Epoch [158/500], Loss: 4.3317\n",
            "Epoch [159/500], Loss: 3.4201\n",
            "Epoch [160/500], Loss: 4.2426\n",
            "Epoch [161/500], Loss: 4.5214\n",
            "Epoch [162/500], Loss: 4.0369\n",
            "Epoch [163/500], Loss: 4.2290\n",
            "Epoch [164/500], Loss: 3.4427\n",
            "Epoch [165/500], Loss: 3.3876\n",
            "Epoch [166/500], Loss: 3.6479\n",
            "Epoch [167/500], Loss: 3.6102\n",
            "Epoch [168/500], Loss: 3.8256\n",
            "Epoch [169/500], Loss: 3.6740\n",
            "Epoch [170/500], Loss: 3.6561\n",
            "Epoch [171/500], Loss: 3.4850\n",
            "Epoch [172/500], Loss: 3.6017\n",
            "Epoch [173/500], Loss: 3.9637\n",
            "Epoch [174/500], Loss: 4.0945\n",
            "Epoch [175/500], Loss: 4.0025\n",
            "Epoch [176/500], Loss: 4.6143\n",
            "Epoch [177/500], Loss: 4.4566\n",
            "Epoch [178/500], Loss: 5.0909\n",
            "Epoch [179/500], Loss: 4.0920\n",
            "Epoch [180/500], Loss: 3.2733\n",
            "Epoch [181/500], Loss: 3.2469\n",
            "Epoch [182/500], Loss: 3.8172\n",
            "Epoch [183/500], Loss: 3.8750\n",
            "Epoch [184/500], Loss: 3.5251\n",
            "Epoch [185/500], Loss: 4.0407\n",
            "Epoch [186/500], Loss: 4.2117\n",
            "Epoch [187/500], Loss: 3.8549\n",
            "Epoch [188/500], Loss: 4.2012\n",
            "Epoch [189/500], Loss: 3.9596\n",
            "Epoch [190/500], Loss: 4.0897\n",
            "Epoch [191/500], Loss: 4.9604\n",
            "Epoch [192/500], Loss: 4.2840\n",
            "Epoch [193/500], Loss: 3.5773\n",
            "Epoch [194/500], Loss: 3.7145\n",
            "Epoch [195/500], Loss: 3.8024\n",
            "Epoch [196/500], Loss: 5.3273\n",
            "Epoch [197/500], Loss: 3.9453\n",
            "Epoch [198/500], Loss: 3.9501\n",
            "Epoch [199/500], Loss: 3.8271\n",
            "Epoch [200/500], Loss: 4.3127\n",
            "Epoch [201/500], Loss: 4.0890\n",
            "Epoch [202/500], Loss: 3.3245\n",
            "Epoch [203/500], Loss: 3.4528\n",
            "Epoch [204/500], Loss: 3.6753\n",
            "Epoch [205/500], Loss: 3.3474\n",
            "Epoch [206/500], Loss: 4.0512\n",
            "Epoch [207/500], Loss: 3.9307\n",
            "Epoch [208/500], Loss: 4.0833\n",
            "Epoch [209/500], Loss: 3.0089\n",
            "Epoch [210/500], Loss: 4.6454\n",
            "Epoch [211/500], Loss: 4.7918\n",
            "Epoch [212/500], Loss: 4.3210\n",
            "Epoch [213/500], Loss: 3.1533\n",
            "Epoch [214/500], Loss: 4.8870\n",
            "Epoch [215/500], Loss: 3.7746\n",
            "Epoch [216/500], Loss: 3.4593\n",
            "Epoch [217/500], Loss: 4.5362\n",
            "Epoch [218/500], Loss: 4.2917\n",
            "Epoch [219/500], Loss: 3.4954\n",
            "Epoch [220/500], Loss: 4.0678\n",
            "Epoch [221/500], Loss: 3.9191\n",
            "Epoch [222/500], Loss: 4.6638\n",
            "Epoch [223/500], Loss: 4.6454\n",
            "Epoch [224/500], Loss: 4.9618\n",
            "Epoch [225/500], Loss: 3.9364\n",
            "Epoch [226/500], Loss: 3.5155\n",
            "Epoch [227/500], Loss: 4.2969\n",
            "Epoch [228/500], Loss: 4.6162\n",
            "Epoch [229/500], Loss: 3.2010\n",
            "Epoch [230/500], Loss: 4.0397\n",
            "Epoch [231/500], Loss: 4.0162\n",
            "Epoch [232/500], Loss: 3.3522\n",
            "Epoch [233/500], Loss: 3.4189\n",
            "Epoch [234/500], Loss: 4.7528\n",
            "Epoch [235/500], Loss: 4.3059\n",
            "Epoch [236/500], Loss: 4.2757\n",
            "Epoch [237/500], Loss: 4.0139\n",
            "Epoch [238/500], Loss: 4.2993\n",
            "Epoch [239/500], Loss: 4.5238\n",
            "Epoch [240/500], Loss: 4.3890\n",
            "Epoch [241/500], Loss: 3.8474\n",
            "Epoch [242/500], Loss: 3.7696\n",
            "Epoch [243/500], Loss: 5.3079\n",
            "Epoch [244/500], Loss: 3.4116\n",
            "Epoch [245/500], Loss: 4.3388\n",
            "Epoch [246/500], Loss: 5.1091\n",
            "Epoch [247/500], Loss: 3.3173\n",
            "Epoch [248/500], Loss: 4.1749\n",
            "Epoch [249/500], Loss: 3.5958\n",
            "Epoch [250/500], Loss: 4.5353\n",
            "Epoch [251/500], Loss: 3.8119\n",
            "Epoch [252/500], Loss: 4.2360\n",
            "Epoch [253/500], Loss: 3.8514\n",
            "Epoch [254/500], Loss: 3.9155\n",
            "Epoch [255/500], Loss: 3.4327\n",
            "Epoch [256/500], Loss: 4.0788\n",
            "Epoch [257/500], Loss: 3.7268\n",
            "Epoch [258/500], Loss: 3.7411\n",
            "Epoch [259/500], Loss: 3.4367\n",
            "Epoch [260/500], Loss: 4.1823\n",
            "Epoch [261/500], Loss: 3.8310\n",
            "Epoch [262/500], Loss: 4.3148\n",
            "Epoch [263/500], Loss: 4.2068\n",
            "Epoch [264/500], Loss: 5.5053\n",
            "Epoch [265/500], Loss: 2.8748\n",
            "Epoch [266/500], Loss: 4.5981\n",
            "Epoch [267/500], Loss: 4.1195\n",
            "Epoch [268/500], Loss: 4.9853\n",
            "Epoch [269/500], Loss: 3.5928\n",
            "Epoch [270/500], Loss: 2.9521\n",
            "Epoch [271/500], Loss: 3.2158\n",
            "Epoch [272/500], Loss: 4.2729\n",
            "Epoch [273/500], Loss: 4.1836\n",
            "Epoch [274/500], Loss: 3.6240\n",
            "Epoch [275/500], Loss: 3.8823\n",
            "Epoch [276/500], Loss: 4.1702\n",
            "Epoch [277/500], Loss: 4.5943\n",
            "Epoch [278/500], Loss: 4.4793\n",
            "Epoch [279/500], Loss: 4.5306\n",
            "Epoch [280/500], Loss: 3.5161\n",
            "Epoch [281/500], Loss: 3.1795\n",
            "Epoch [282/500], Loss: 3.4769\n",
            "Epoch [283/500], Loss: 5.5413\n",
            "Epoch [284/500], Loss: 3.8141\n",
            "Epoch [285/500], Loss: 3.6178\n",
            "Epoch [286/500], Loss: 3.7150\n",
            "Epoch [287/500], Loss: 4.1849\n",
            "Epoch [288/500], Loss: 3.2968\n",
            "Epoch [289/500], Loss: 4.8509\n",
            "Epoch [290/500], Loss: 4.2180\n",
            "Epoch [291/500], Loss: 4.8181\n",
            "Epoch [292/500], Loss: 3.4386\n",
            "Epoch [293/500], Loss: 4.9775\n",
            "Epoch [294/500], Loss: 3.8798\n",
            "Epoch [295/500], Loss: 4.3220\n",
            "Epoch [296/500], Loss: 4.3470\n",
            "Epoch [297/500], Loss: 3.9212\n",
            "Epoch [298/500], Loss: 4.0221\n",
            "Epoch [299/500], Loss: 3.5463\n",
            "Epoch [300/500], Loss: 3.2796\n",
            "Epoch [301/500], Loss: 3.9510\n",
            "Epoch [302/500], Loss: 3.9869\n",
            "Epoch [303/500], Loss: 3.6068\n",
            "Epoch [304/500], Loss: 3.5314\n",
            "Epoch [305/500], Loss: 4.2741\n",
            "Epoch [306/500], Loss: 3.1091\n",
            "Epoch [307/500], Loss: 4.7808\n",
            "Epoch [308/500], Loss: 4.1896\n",
            "Epoch [309/500], Loss: 4.2270\n",
            "Epoch [310/500], Loss: 3.2191\n",
            "Epoch [311/500], Loss: 4.1556\n",
            "Epoch [312/500], Loss: 3.3428\n",
            "Epoch [313/500], Loss: 4.3583\n",
            "Epoch [314/500], Loss: 5.1640\n",
            "Epoch [315/500], Loss: 4.6061\n",
            "Epoch [316/500], Loss: 3.5055\n",
            "Epoch [317/500], Loss: 4.6355\n",
            "Epoch [318/500], Loss: 4.1097\n",
            "Epoch [319/500], Loss: 2.8714\n",
            "Epoch [320/500], Loss: 4.5221\n",
            "Epoch [321/500], Loss: 4.3163\n",
            "Epoch [322/500], Loss: 4.1171\n",
            "Epoch [323/500], Loss: 3.8592\n",
            "Epoch [324/500], Loss: 4.3193\n",
            "Epoch [325/500], Loss: 5.2160\n",
            "Epoch [326/500], Loss: 3.3628\n",
            "Epoch [327/500], Loss: 4.3700\n",
            "Epoch [328/500], Loss: 3.0105\n",
            "Epoch [329/500], Loss: 3.2461\n",
            "Epoch [330/500], Loss: 3.7209\n",
            "Epoch [331/500], Loss: 4.1991\n",
            "Epoch [332/500], Loss: 4.2774\n",
            "Epoch [333/500], Loss: 4.1933\n",
            "Epoch [334/500], Loss: 3.8062\n",
            "Epoch [335/500], Loss: 4.3867\n",
            "Epoch [336/500], Loss: 4.6371\n",
            "Epoch [337/500], Loss: 4.0326\n",
            "Epoch [338/500], Loss: 2.8912\n",
            "Epoch [339/500], Loss: 4.1006\n",
            "Epoch [340/500], Loss: 4.4689\n",
            "Epoch [341/500], Loss: 4.0111\n",
            "Epoch [342/500], Loss: 3.8005\n",
            "Epoch [343/500], Loss: 5.6223\n",
            "Epoch [344/500], Loss: 4.3850\n",
            "Epoch [345/500], Loss: 4.8178\n",
            "Epoch [346/500], Loss: 3.1512\n",
            "Epoch [347/500], Loss: 4.7092\n",
            "Epoch [348/500], Loss: 3.9066\n",
            "Epoch [349/500], Loss: 3.7563\n",
            "Epoch [350/500], Loss: 4.2800\n",
            "Epoch [351/500], Loss: 3.8756\n",
            "Epoch [352/500], Loss: 4.2862\n",
            "Epoch [353/500], Loss: 5.7593\n",
            "Epoch [354/500], Loss: 3.1836\n",
            "Epoch [355/500], Loss: 3.7434\n",
            "Epoch [356/500], Loss: 3.9048\n",
            "Epoch [357/500], Loss: 4.3049\n",
            "Epoch [358/500], Loss: 3.4372\n",
            "Epoch [359/500], Loss: 4.1367\n",
            "Epoch [360/500], Loss: 4.0810\n",
            "Epoch [361/500], Loss: 3.8203\n",
            "Epoch [362/500], Loss: 5.1440\n",
            "Epoch [363/500], Loss: 4.2844\n",
            "Epoch [364/500], Loss: 4.0256\n",
            "Epoch [365/500], Loss: 4.8803\n",
            "Epoch [366/500], Loss: 3.0215\n",
            "Epoch [367/500], Loss: 4.3934\n",
            "Epoch [368/500], Loss: 4.3930\n",
            "Epoch [369/500], Loss: 4.2922\n",
            "Epoch [370/500], Loss: 4.6851\n",
            "Epoch [371/500], Loss: 3.8464\n",
            "Epoch [372/500], Loss: 4.3999\n",
            "Epoch [373/500], Loss: 3.5147\n",
            "Epoch [374/500], Loss: 3.3434\n",
            "Epoch [375/500], Loss: 3.6594\n",
            "Epoch [376/500], Loss: 3.8072\n",
            "Epoch [377/500], Loss: 3.8550\n",
            "Epoch [378/500], Loss: 4.4556\n",
            "Epoch [379/500], Loss: 3.0876\n",
            "Epoch [380/500], Loss: 4.6379\n",
            "Epoch [381/500], Loss: 3.4531\n",
            "Epoch [382/500], Loss: 4.0475\n",
            "Epoch [383/500], Loss: 3.9356\n",
            "Epoch [384/500], Loss: 3.6493\n",
            "Epoch [385/500], Loss: 3.7027\n",
            "Epoch [386/500], Loss: 4.8951\n",
            "Epoch [387/500], Loss: 3.4380\n",
            "Epoch [388/500], Loss: 4.3490\n",
            "Epoch [389/500], Loss: 4.0246\n",
            "Epoch [390/500], Loss: 3.5166\n",
            "Epoch [391/500], Loss: 4.9183\n",
            "Epoch [392/500], Loss: 4.0133\n",
            "Epoch [393/500], Loss: 4.7646\n",
            "Epoch [394/500], Loss: 3.7433\n",
            "Epoch [395/500], Loss: 3.6170\n",
            "Epoch [396/500], Loss: 3.7321\n",
            "Epoch [397/500], Loss: 3.5106\n",
            "Epoch [398/500], Loss: 4.5092\n",
            "Epoch [399/500], Loss: 2.9221\n",
            "Epoch [400/500], Loss: 3.9722\n",
            "Epoch [401/500], Loss: 4.4447\n",
            "Epoch [402/500], Loss: 4.0363\n",
            "Epoch [403/500], Loss: 5.0434\n",
            "Epoch [404/500], Loss: 3.5210\n",
            "Epoch [405/500], Loss: 4.2979\n",
            "Epoch [406/500], Loss: 4.0281\n",
            "Epoch [407/500], Loss: 4.3117\n",
            "Epoch [408/500], Loss: 3.3999\n",
            "Epoch [409/500], Loss: 4.4481\n",
            "Epoch [410/500], Loss: 4.2896\n",
            "Epoch [411/500], Loss: 3.7449\n",
            "Epoch [412/500], Loss: 4.1118\n",
            "Epoch [413/500], Loss: 4.4923\n",
            "Epoch [414/500], Loss: 3.2722\n",
            "Epoch [415/500], Loss: 4.7897\n",
            "Epoch [416/500], Loss: 3.6131\n",
            "Epoch [417/500], Loss: 3.9468\n",
            "Epoch [418/500], Loss: 4.2653\n",
            "Epoch [419/500], Loss: 4.8331\n",
            "Epoch [420/500], Loss: 3.7429\n",
            "Epoch [421/500], Loss: 3.9682\n",
            "Epoch [422/500], Loss: 4.2738\n",
            "Epoch [423/500], Loss: 3.2508\n",
            "Epoch [424/500], Loss: 3.9604\n",
            "Epoch [425/500], Loss: 3.6579\n",
            "Epoch [426/500], Loss: 3.6907\n",
            "Epoch [427/500], Loss: 4.2861\n",
            "Epoch [428/500], Loss: 4.2528\n",
            "Epoch [429/500], Loss: 3.4337\n",
            "Epoch [430/500], Loss: 3.9306\n",
            "Epoch [431/500], Loss: 5.1585\n",
            "Epoch [432/500], Loss: 4.1127\n",
            "Epoch [433/500], Loss: 3.6035\n",
            "Epoch [434/500], Loss: 4.7310\n",
            "Epoch [435/500], Loss: 4.0547\n",
            "Epoch [436/500], Loss: 5.3675\n",
            "Epoch [437/500], Loss: 4.0614\n",
            "Epoch [438/500], Loss: 4.6991\n",
            "Epoch [439/500], Loss: 3.9159\n",
            "Epoch [440/500], Loss: 4.2041\n",
            "Epoch [441/500], Loss: 4.3571\n",
            "Epoch [442/500], Loss: 4.0660\n",
            "Epoch [443/500], Loss: 3.9629\n",
            "Epoch [444/500], Loss: 5.2141\n",
            "Epoch [445/500], Loss: 4.1557\n",
            "Epoch [446/500], Loss: 3.9083\n",
            "Epoch [447/500], Loss: 4.6146\n",
            "Epoch [448/500], Loss: 4.1319\n",
            "Epoch [449/500], Loss: 3.9248\n",
            "Epoch [450/500], Loss: 3.9299\n",
            "Epoch [451/500], Loss: 3.7563\n",
            "Epoch [452/500], Loss: 4.0443\n",
            "Epoch [453/500], Loss: 4.6069\n",
            "Epoch [454/500], Loss: 3.9922\n",
            "Epoch [455/500], Loss: 3.4127\n",
            "Epoch [456/500], Loss: 4.1268\n",
            "Epoch [457/500], Loss: 3.8654\n",
            "Epoch [458/500], Loss: 4.3192\n",
            "Epoch [459/500], Loss: 3.7918\n",
            "Epoch [460/500], Loss: 4.0609\n",
            "Epoch [461/500], Loss: 5.0321\n",
            "Epoch [462/500], Loss: 3.4909\n",
            "Epoch [463/500], Loss: 5.0069\n",
            "Epoch [464/500], Loss: 4.2147\n",
            "Epoch [465/500], Loss: 3.0784\n",
            "Epoch [466/500], Loss: 3.7617\n",
            "Epoch [467/500], Loss: 3.7203\n",
            "Epoch [468/500], Loss: 4.0790\n",
            "Epoch [469/500], Loss: 4.4003\n",
            "Epoch [470/500], Loss: 4.2847\n",
            "Epoch [471/500], Loss: 4.0089\n",
            "Epoch [472/500], Loss: 3.4322\n",
            "Epoch [473/500], Loss: 4.2767\n",
            "Epoch [474/500], Loss: 4.0977\n",
            "Epoch [475/500], Loss: 3.9828\n",
            "Epoch [476/500], Loss: 4.5541\n",
            "Epoch [477/500], Loss: 2.7178\n",
            "Epoch [478/500], Loss: 3.9140\n",
            "Epoch [479/500], Loss: 3.6437\n",
            "Epoch [480/500], Loss: 5.3994\n",
            "Epoch [481/500], Loss: 4.6406\n",
            "Epoch [482/500], Loss: 4.4268\n",
            "Epoch [483/500], Loss: 4.1840\n",
            "Epoch [484/500], Loss: 4.0163\n",
            "Epoch [485/500], Loss: 3.7454\n",
            "Epoch [486/500], Loss: 4.0029\n",
            "Epoch [487/500], Loss: 3.6258\n",
            "Epoch [488/500], Loss: 3.6926\n",
            "Epoch [489/500], Loss: 4.2797\n",
            "Epoch [490/500], Loss: 4.4453\n",
            "Epoch [491/500], Loss: 3.2251\n",
            "Epoch [492/500], Loss: 3.2102\n",
            "Epoch [493/500], Loss: 4.7090\n",
            "Epoch [494/500], Loss: 3.8153\n",
            "Epoch [495/500], Loss: 3.2042\n",
            "Epoch [496/500], Loss: 4.4461\n",
            "Epoch [497/500], Loss: 3.3788\n",
            "Epoch [498/500], Loss: 4.8383\n",
            "Epoch [499/500], Loss: 4.5207\n",
            "Epoch [500/500], Loss: 4.2893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Où sont alors stockées les estimations des  $\\theta_k$ ? Les extraire du réseau *mySimpleNet* dans la cellule suivante."
      ],
      "metadata": {
        "id": "OZwKogEEp2Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Les estimation des thetak sont stockés dans les paramètres de la couche linéaire du modèle\n",
        " #(de l'objet mySimpleNet). On peut accéder au poids via .linear.weight et au biais via .linear.bias."
      ],
      "metadata": {
        "id": "EjgWp1y1rseb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraire les trois poids\n",
        "weights = mySimpleNet.linear.weight.data.numpy()\n",
        "\n",
        "# Extraire le biais (θ0)\n",
        "bias = mySimpleNet.linear.bias.data.numpy()\n",
        "\n",
        "print(\"Poids (θ1, θ2, θ3) :\", weights)\n",
        "print(\"Biais (θ0) :\", bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8NrryHtbbGv",
        "outputId": "42463b6e-2d92-48bf-bdb8-151602cf484d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Poids (θ1, θ2, θ3) : [[3.4428918 3.5909214 7.174001 ]]\n",
            "Biais (θ0) : [17.429174]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** Tester ces estimations sur le jeu de test et comparer avec celles de la question 1. Commentez."
      ],
      "metadata": {
        "id": "pEB-V-oOrJED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "\n",
        "# Préparer les données de test\n",
        "X_test = test_set['inputs']\n",
        "y_test = test_set['targets']\n",
        "# Prédictions avec le réseau de neurones\n",
        "X_test_tensor = torch.tensor(X_test).float()\n",
        "predictions_nn = mySimpleNet(X_test_tensor).detach().numpy()\n",
        "# Prédiction avec la régression linéaire\n",
        "model = LinearRegression()\n",
        "model.fit(train_set['inputs'], train_set['targets'])\n",
        "predictions_lr = model.predict(X_test)\n",
        "# Comparaison des performances\n",
        "mse_nn = mean_squared_error(y_test, predictions_nn)\n",
        "mse_lr = mean_squared_error(y_test, predictions_lr)\n",
        "\n",
        "print(\"MSE du réseau de neurones:\", mse_nn)\n",
        "print(\"MSE de la régression linéaire:\", mse_lr)\n",
        "\n",
        "#Les moyennes sont très similaires ce qui montre que les deux\n",
        "# approches donnent des performances identiques en termes de généralisation.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGslGWNHcCuk",
        "outputId": "8ac95015-423a-4d78-f8f3-842e522afce4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE du réseau de neurones: 4.055477822302178\n",
            "MSE de la régression linéaire: 4.055641510168173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "VvV2jIrBNtzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 2** *Champ réceptif et prédiction causale*"
      ],
      "metadata": {
        "id": "CpRvXCaAtsIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le réseau défini dans la cellule suivante est utilisé pour faire le lien entre les valeurs $(x_{t' \\leq t})$ d'une série temporelle d'entrée et la valeur présente $y_t$ d'une série temporelle cible."
      ],
      "metadata": {
        "id": "8JG9wTfK5TBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from utils_exercices import Outconv, Up_causal, Down_causal\n",
        "\n",
        "class Double_conv_causal(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2, with causal convolutions that preserve input size'''\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, dilation=1):\n",
        "        super(Double_conv_causal, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation = dilation\n",
        "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn1 = nn.BatchNorm1d(out_ch)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm1d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class causalFCN(nn.Module):\n",
        "    def __init__(self, dilation=1):\n",
        "        super(causalFCN, self).__init__()\n",
        "        size = 64\n",
        "        n_channels = 1\n",
        "        n_classes = 1\n",
        "        self.inc = Double_conv_causal(n_channels, size)\n",
        "        self.down1 = Down_causal(size, 2*size)\n",
        "        self.down2 = Down_causal(2*size, 4*size)\n",
        "        self.down3 = Down_causal(4*size, 8*size, pooling_kernel_size=5, pooling_stride=5)\n",
        "        self.down4 = Down_causal(8*size, 4*size, pooling=False, dilation=2)\n",
        "        self.up2 = Up_causal(4*size, 2*size, kernel_size=5, stride=5)\n",
        "        self.up3 = Up_causal(2*size, size)\n",
        "        self.up4 = Up_causal(size, size)\n",
        "        self.outc = Outconv(size, n_classes)\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up2(x5, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return x\n",
        "\n",
        "# Exemple d'utilisation\n",
        "model = causalFCN()\n",
        "# Série temporelle d'entrée (x_t):\n",
        "input_tensor1 = torch.rand(1, 1, 10000)\n",
        "# Série temporelle en sortie f(x_t):\n",
        "output = model(input_tensor1)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "fIbU1EJT1MM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dd40299-d5c4-4585-e122-5995016aee84"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** De quel type de réseau de neurones s'agit-il ? Combien de paramètres la couche self.Down1 compte-t-elle (à faire à la main) ?\n",
        "Combien de paramètres le réseau entier compte-t-il (avec un peu de code) ?"
      ],
      "metadata": {
        "id": "-mNnsYU-7R7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Il s'agit FCN , fully convolutional Network\n",
        "#, plus précisément un réseau de neurones adapté pour le traitement des séries temporelles."
      ],
      "metadata": {
        "id": "7GCAFIi1eQi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nb de paramètres dans self.Down1: (calcul \"à la main\")\n",
        "# La formule = entrée * taille_noyau * sortie + sortie\n",
        "# ici : entrée = 64 canaux , sortie = 2*64 = 128 canaux et taille_noyau = 3\n",
        "# nb_para = (64*3*128) + 128 = 24 704.\n",
        "\n",
        "\n",
        "# Nb de paramètres au total: 2872641 voir calcul si dessous"
      ],
      "metadata": {
        "id": "qlYxUf6U9vH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model = causalFCN()\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"Nombre total de paramètres : {total_params}\")"
      ],
      "metadata": {
        "id": "qp1LxBDTgFN4",
        "outputId": "37236f3d-0a37-4e9f-f581-4eaa7205955d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre total de paramètres : 2872641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Par quels mécanismes la taille du vecteur d'entrée est-elle réduite ? Comment est-elle restituée dans la deuxième partie du réseau ?"
      ],
      "metadata": {
        "id": "I4D46A0-8LaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Les mécanismes pour réduire la taille du vecteur sont :\n",
        "# Down_causal : des couches de convolution causales ( extraient des caractéristiques temporelles tout en respectant la causalité)\n",
        "# et de pooling(stride) en  diminuant la longueur de la séquence, dans les couches descendantes.\n",
        "# Pour la restitution via Upsampling : les couches Up_causal utilisent des méthodes de upsampling.\n",
        "# Chaque couche Up_causal utilise une convolution avec un noyau et un stride,\n",
        "# et fusionne les informations des couches correspondantes de descente pour restaurer progressivement\n",
        "# la résolution du vecteur d'entrée.\n"
      ],
      "metadata": {
        "id": "WVSccV5GghpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Par quels mécanismes le champ réceptif est-il augmenté ? Préciser par un calcul la taille du champ réceptif en sortie de *self.inc*."
      ],
      "metadata": {
        "id": "SVNeFnm88yV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Le champ réceptif en sortie de self.inc est de 5 éléments.\n",
        "# Cela résulte de deux convolutions successives avec un noyau de taille 3 et une dilatation de 1,\n",
        "# chacune ajoutant 3 éléments au champ réceptif, donnant ainsi un total de 1 + (3 -1)* 1 * 2 = 5.\n"
      ],
      "metadata": {
        "id": "Ql5MvoXyiYCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Par un bout de code, déterminer empiriquement la taille du champ réceptif associé à la composante $y_{5000}$ du vecteur de sortie. (Indice: considérer les sorties associées à deux inputs qui ne diffèrent que par une composante...)"
      ],
      "metadata": {
        "id": "TVVcBPuA9EP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model = causalFCN()\n",
        "\n",
        "# Créer deux entrées\n",
        "input1 = torch.zeros(1, 1, 10000)\n",
        "input2 = torch.zeros(1, 1, 10000)\n",
        "# modifier la composante à l'indice 5000\n",
        "input2[0, 0, 5000] = 1\n",
        "\n",
        "# Calculer les sorties\n",
        "output1 = model(input1)\n",
        "output2 = model(input2)\n",
        "\n",
        "indices_diff = torch.nonzero(output1 - output2)\n",
        "\n",
        "# Le champ réceptif est la plage d'indices où les sorties diffèrent\n",
        "receptive_field_size = indices_diff[-1, -1] - indices_diff[0, -1] + 1\n",
        "\n",
        "print(\"La taille du champ réceptif pour y_5000 est :\", receptive_field_size.item())"
      ],
      "metadata": {
        "id": "69WMWCSZAg5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b417ffc-1084-4ab7-dcda-ebeeed33b5ef"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La taille du champ réceptif pour y_5000 est : 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** $y_{5000}$ dépend-elle des composantes $x_{t, \\space t > 5000}$ ? Justifier de manière empirique puis préciser la partie du code de Double_conv_causal qui garantit cette propriété de \"causalité\" en justifiant.  \n",
        "\n"
      ],
      "metadata": {
        "id": "gZ37skwm-Vpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# Instancier le modèle\n",
        "model = causalFCN()\n",
        "\n",
        "input1 = torch.zeros(1, 1, 10000)\n",
        "input2 = torch.zeros(1, 1, 10000)\n",
        "input2[0, 0, 5001:] = torch.rand(4999)\n",
        "\n",
        "# Calculer les sorties correspondantes\n",
        "output1 = model(input1)\n",
        "output2 = model(input2)\n",
        "\n",
        "# Comparer la valeur de y_5000 dans les deux sorties\n",
        "print(\"y_5000_in1:\", output1[0, 0, 5000].item())\n",
        "print(\"y_5000_in2 :\", output2[0, 0, 5000].item())"
      ],
      "metadata": {
        "id": "PeooRYE-ATGt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea0161a6-c8ed-4632-c0d2-c4554bca33c8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_5000_in1: 0.027746466919779778\n",
            "y_5000_in2 : 0.25076577067375183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4dy1VbRcjMgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "qV52tusgNn6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "Exercice 3: \"Ranknet loss\""
      ],
      "metadata": {
        "id": "bm-sRzmfqc2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un [article récent](https://https://arxiv.org/abs/2403.14144) revient sur les progrès en matière de learning to rank. En voilà un extrait :"
      ],
      "metadata": {
        "id": "Wl8wUjsSM57D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/nanopiero/exam_2025/refs/heads/main/utils/png_exercice3.PNG?\n",
        "\" alt=\"extrait d'un article\" width=\"800\">"
      ],
      "metadata": {
        "id": "SDZUXMlSDpoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Qu'est-ce que les auteurs appellent \"positive samples\" et \"negative samples\" ? Donner un exemple."
      ],
      "metadata": {
        "id": "9NzV1PbMNyuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# positive samples =\n",
        "# negative samples ="
      ],
      "metadata": {
        "id": "FK6sEkDRkgAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tjx8jhwnkd1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans l'expression de $\\mathcal{L}_{RankNet}$, d'où proviennent les $z_i$ ? Que représentent-ils ?  "
      ],
      "metadata": {
        "id": "yIKQ5Eo9OnPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Pourquoi cette expression conduit-elle à ce que, après apprentissage, \"the estimated\n",
        "value of positive samples is greater than that of negative samples\n",
        "for each pair of positive/negative samples\" ?"
      ],
      "metadata": {
        "id": "r74fWiyvPb7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Dans le cadre d'une approche par deep learning, quels termes utilise-t-on pour qualifier les réseaux de neurones exploités et la modalité suivant laquelle ils sont entraînés ?"
      ],
      "metadata": {
        "id": "pk1EIi_VVi3R"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}